#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
City Look Dissertation v2
stream_gvi_processor.py - æµå¼GVIå¤„ç†ï¼ˆä¸ä¿å­˜å›¾ç‰‡ï¼‰

ç›´æ¥ä»Mapillaryä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜ï¼Œè®¡ç®—GVIååªä¿å­˜ç»“æœ
å¤§å¹…å‡å°‘å­˜å‚¨éœ€æ±‚ï¼Œä»100GBé™åˆ°<1GB

ä½œè€…ï¼šYaxin
æ—¥æœŸï¼š2025-01-14
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import requests
from io import BytesIO
from PIL import Image
import torch
from tqdm import tqdm
import logging
import json
import time
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

# ============================================
# Windowsé…ç½®
# ============================================

class Config:
    """Windowsé¡¹ç›®é…ç½®"""
    
    # Windowsè·¯å¾„
    BASE_DIR = Path(r"C:\Users\z1782\OneDrive - University College London\Attachments\004\methodology\Dissertation_v2")
    
    # æ•°æ®è·¯å¾„
    IMD_DATA = BASE_DIR / "data" / "raw" / "IMD2019_London.xlsx"
    SELECTED_LSOAS = BASE_DIR / "data" / "processed" / "selected_lsoas.csv"
    
    # è¾“å‡ºè·¯å¾„ï¼ˆåªä¿å­˜ç»“æœï¼Œä¸ä¿å­˜å›¾ç‰‡ï¼‰
    OUTPUT_DIR = BASE_DIR / "output" / "stream_gvi"
    RESULTS_DIR = BASE_DIR / "data" / "processed"
    
    # Mapillaryé…ç½®
    MAPILLARY_TOKEN = "MLY|9922859457805691|cef02444f32c339cf09761b104ca4bb5"
    MAPILLARY_API = "https://graph.mapillary.com"
    
    # å›¾ç‰‡æ”¶é›†å‚æ•°
    TARGET_IMAGES = 100  # ç›®æ ‡100å¼ 
    MIN_IMAGES = 60     # æœ€å°‘80å¼ 
    MAX_IMAGES = 100     # æœ€å¤š100å¼ 
    
    # Inner London Boroughs
    INNER_LONDON_BOROUGHS = [
        "Camden", "Greenwich", "Hackney", 
        "Hammersmith and Fulham", "Islington",
        "Kensington and Chelsea", "Lambeth", 
        "Lewisham", "Southwark", "Tower Hamlets",
        "Wandsworth", "Westminster"
    ]
    
    # å¤„ç†å‚æ•°
    BATCH_SIZE = 10      # æ¯æ‰¹å¤„ç†10ä¸ªLSOAï¼ˆå†…å­˜å‹å¥½ï¼‰
    MAX_WORKERS = 4      # å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°

# ============================================
# Mapillaryæµå¼ä¸‹è½½å™¨
# ============================================

class MapillaryStreamer:
    """Mapillaryæµå¼å›¾ç‰‡å¤„ç†å™¨"""
    
    def __init__(self, token):
        self.token = token
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'OAuth {self.token}'
        })
        
    def search_images_in_bbox(self, bbox, limit=100):
        """åœ¨è¾¹ç•Œæ¡†å†…æœç´¢å›¾ç‰‡"""
        url = f"{Config.MAPILLARY_API}/images"
        
        params = {
            'bbox': f'{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}',
            'fields': 'id,captured_at,thumb_2048_url,computed_geometry',
            'limit': min(limit, 500)  # APIé™åˆ¶
        }
        
        try:
            response = self.session.get(url, params=params, timeout=30)
            if response.status_code == 200:
                data = response.json()
                return data.get('data', [])
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            
        return []
    
    def download_image_to_memory(self, image_url):
        """ä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜ï¼ˆä¸ä¿å­˜ï¼‰"""
        try:
            response = self.session.get(image_url, timeout=30)
            if response.status_code == 200:
                # ç›´æ¥è¿”å›PIL Imageå¯¹è±¡
                return Image.open(BytesIO(response.content)).convert('RGB')
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥: {e}")
        
        return None

# ============================================
# è½»é‡çº§GVIè®¡ç®—å™¨
# ============================================

class StreamGVICalculator:
    """æµå¼GVIè®¡ç®—å™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, use_gpu=False):
        self.device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½SegFormerï¼ˆè½»é‡çº§ï¼‰
        from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor
        
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
        model_name = "nvidia/segformer-b0-finetuned-ade-512-512"  # b0æ›´å°æ›´å¿«
        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # ADE20Kæ¤è¢«ç±»åˆ«
        self.vegetation_classes = [4, 9, 17, 29, 46, 66, 90]  # tree, grass, plantç­‰
    
    def calculate_gvi_from_pil(self, pil_image):
        """ä»PILå›¾ç‰‡ç›´æ¥è®¡ç®—GVIï¼ˆä¸ä¿å­˜ï¼‰"""
        try:
            # é¢„å¤„ç†
            inputs = self.feature_extractor(images=pil_image, return_tensors="pt")
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs.to(self.device))
                
                # è·å–é¢„æµ‹
                logits = outputs.logits
                pred = logits.argmax(dim=1).cpu().numpy()[0]
            
            # è®¡ç®—æ¤è¢«åƒç´ 
            vegetation_mask = np.isin(pred, self.vegetation_classes)
            
            # è®¡ç®—GVI
            total_pixels = pred.size
            vegetation_pixels = np.sum(vegetation_mask)
            gvi = (vegetation_pixels / total_pixels) * 100
            
            # æ¸…ç†å†…å­˜
            del inputs, outputs, logits, pred
            torch.cuda.empty_cache() if self.device.type == 'cuda' else None
            
            return {
                'gvi': gvi,
                'vegetation_pixels': int(vegetation_pixels),
                'total_pixels': int(total_pixels)
            }
            
        except Exception as e:
            print(f"GVIè®¡ç®—å¤±è´¥: {e}")
            return None

# ============================================
# æµå¼LSOAå¤„ç†å™¨
# ============================================

class StreamLSOAProcessor:
    """æµå¼å¤„ç†å•ä¸ªLSOA"""
    
    def __init__(self):
        self.streamer = MapillaryStreamer(Config.MAPILLARY_TOKEN)
        self.calculator = StreamGVICalculator(use_gpu=False)  # Windowsä¸Šé€šå¸¸ç”¨CPU
        
    def process_lsoa(self, lsoa_code, bbox):
        """å¤„ç†å•ä¸ªLSOAï¼ˆä¸ä¿å­˜å›¾ç‰‡ï¼‰"""
        
        print(f"\nå¤„ç† {lsoa_code}...")
        
        # Step 1: æœç´¢å›¾ç‰‡
        images_metadata = self.streamer.search_images_in_bbox(bbox, limit=Config.MAX_IMAGES)
        
        if len(images_metadata) < Config.MIN_IMAGES:
            print(f"âš ï¸ {lsoa_code} åªæ‰¾åˆ° {len(images_metadata)} å¼ å›¾ç‰‡ï¼ˆéœ€è¦è‡³å°‘{Config.MIN_IMAGES}å¼ ï¼‰")
            return None
        
        # é™åˆ¶åˆ°æœ€å¤š100å¼ 
        images_metadata = images_metadata[:Config.MAX_IMAGES]
        print(f"æ‰¾åˆ° {len(images_metadata)} å¼ å›¾ç‰‡")
        
        # Step 2: æµå¼å¤„ç†æ¯å¼ å›¾ç‰‡
        gvi_results = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:  # é™åˆ¶å¹¶å‘é¿å…å†…å­˜æº¢å‡º
            for img_meta in tqdm(images_metadata, desc=f"è®¡ç®—{lsoa_code}çš„GVI"):
                
                # è·å–å›¾ç‰‡URL
                image_url = img_meta.get('thumb_2048_url')
                if not image_url:
                    continue
                
                # ä¸‹è½½åˆ°å†…å­˜
                pil_image = self.streamer.download_image_to_memory(image_url)
                if pil_image is None:
                    continue
                
                # è®¡ç®—GVI
                gvi_result = self.calculator.calculate_gvi_from_pil(pil_image)
                if gvi_result:
                    gvi_result['image_id'] = img_meta['id']
                    gvi_result['captured_at'] = img_meta.get('captured_at', '')
                    gvi_results.append(gvi_result)
                
                # ç«‹å³é‡Šæ”¾å›¾ç‰‡å†…å­˜
                del pil_image
        
        # Step 3: æ±‡æ€»ç»“æœ
        if len(gvi_results) >= Config.MIN_IMAGES:
            gvi_values = [r['gvi'] for r in gvi_results]
            
            summary = {
                'lsoa_code': lsoa_code,
                'n_images': len(gvi_results),
                'mean_gvi': np.mean(gvi_values),
                'median_gvi': np.median(gvi_values),
                'std_gvi': np.std(gvi_values),
                'min_gvi': np.min(gvi_values),
                'max_gvi': np.max(gvi_values),
                'q25_gvi': np.percentile(gvi_values, 25),
                'q75_gvi': np.percentile(gvi_values, 75),
                'processing_time': datetime.now().isoformat()
            }
            
            print(f"âœ… {lsoa_code} å®Œæˆ: å¹³å‡GVI={summary['mean_gvi']:.2f}%")
            
            # å¯é€‰ï¼šä¿å­˜è¯¦ç»†ç»“æœï¼ˆCSVå¾ˆå°ï¼‰
            details_df = pd.DataFrame(gvi_results)
            details_df['lsoa_code'] = lsoa_code
            
            return summary, details_df
        
        else:
            print(f"âŒ {lsoa_code} å¤„ç†å¤±è´¥ï¼šæœ‰æ•ˆå›¾ç‰‡ä¸è¶³")
            return None

# ============================================
# æ‰¹é‡æµå¼å¤„ç†ç®¡é“
# ============================================

class StreamPipeline:
    """æµå¼å¤„ç†ç®¡é“"""
    
    def __init__(self):
        self.processor = StreamLSOAProcessor()
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Config.BASE_DIR / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"stream_{datetime.now():%Y%m%d}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_lsoa_boundaries(self):
        """åŠ è½½LSOAè¾¹ç•Œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œéœ€è¦ä½ çš„LSOAè¾¹ç•Œæ•°æ®
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        
        # è¯»å–IMDæ•°æ®è·å–LSOAåˆ—è¡¨
        imd_data = pd.read_excel(Config.IMD_DATA)
        inner_lsoas = imd_data[imd_data['Borough'].isin(Config.INNER_LONDON_BOROUGHS)]
        
        # éœ€è¦ä»shapefileè·å–å®é™…è¾¹ç•Œ
        # è¿™é‡Œç”¨ç®€åŒ–çš„è¾¹ç•Œæ¡†ï¼ˆéœ€è¦å®é™…åæ ‡ï¼‰
        boundaries = []
        for _, row in inner_lsoas.iterrows():
            # æ¨¡æ‹Ÿè¾¹ç•Œæ¡† [min_lon, min_lat, max_lon, max_lat]
            # å®é™…éœ€è¦ä»shapefileè¯»å–
            bbox = [-0.1, 51.5, -0.05, 51.55]  # ç¤ºä¾‹åæ ‡
            boundaries.append({
                'lsoa_code': row['LSOA code (2011)'],
                'bbox': bbox
            })
        
        return boundaries[:100]  # å…ˆå¤„ç†100ä¸ª
    
    def run(self, target_count=100):
        """è¿è¡Œæµå¼å¤„ç†"""
        
        self.logger.info(f"å¼€å§‹æµå¼å¤„ç† {target_count} ä¸ªLSOA")
        
        # åŠ è½½LSOAè¾¹ç•Œ
        lsoa_boundaries = self.load_lsoa_boundaries()[:target_count]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†æ¯ä¸ªLSOA
        all_summaries = []
        all_details = []
        
        for i, lsoa_info in enumerate(lsoa_boundaries):
            print(f"\nè¿›åº¦: {i+1}/{len(lsoa_boundaries)}")
            
            result = self.processor.process_lsoa(
                lsoa_info['lsoa_code'],
                lsoa_info['bbox']
            )
            
            if result:
                summary, details = result
                all_summaries.append(summary)
                all_details.append(details)
                
                # æ¯10ä¸ªLSOAä¿å­˜ä¸€æ¬¡ï¼ˆé¿å…ä¸¢å¤±ï¼‰
                if (i + 1) % 10 == 0:
                    self.save_intermediate_results(all_summaries, all_details)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results(all_summaries, all_details)
        
        self.logger.info("âœ… æµå¼å¤„ç†å®Œæˆï¼")
        
        return all_summaries
    
    def save_intermediate_results(self, summaries, details):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        temp_dir = Config.OUTPUT_DIR / "temp"
        temp_dir.mkdir(exist_ok=True)
        
        pd.DataFrame(summaries).to_csv(
            temp_dir / f"temp_summary_{datetime.now():%H%M}.csv", 
            index=False
        )
        
        if details:
            pd.concat(details).to_csv(
                temp_dir / f"temp_details_{datetime.now():%H%M}.csv",
                index=False
            )
    
    def save_final_results(self, summaries, details):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        
        # æ±‡æ€»ç»“æœ
        summary_df = pd.DataFrame(summaries)
        summary_df.to_csv(
            Config.RESULTS_DIR / "stream_gvi_summary.csv",
            index=False
        )
        
        # è¯¦ç»†ç»“æœï¼ˆå¯é€‰ï¼‰
        if details:
            details_df = pd.concat(details, ignore_index=True)
            details_df.to_csv(
                Config.RESULTS_DIR / "stream_gvi_details.csv",
                index=False
            )
        
        # ç»Ÿè®¡æŠ¥å‘Š
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  LSOAæ•°é‡: {len(summary_df)}")
        print(f"  å¹³å‡GVI: {summary_df['mean_gvi'].mean():.2f}%")
        print(f"  GVIèŒƒå›´: {summary_df['mean_gvi'].min():.2f}% - {summary_df['mean_gvi'].max():.2f}%")
        print(f"  æ€»å›¾ç‰‡æ•°: {summary_df['n_images'].sum()}")
        print("="*50)

# ============================================
# ä¸»ç¨‹åº
# ============================================

def main():
    """ä¸»å‡½æ•°"""
    
    import argparse
    parser = argparse.ArgumentParser(description='æµå¼GVIå¤„ç†')
    parser.add_argument('--target', type=int, default=100,
                       help='ç›®æ ‡LSOAæ•°é‡')
    parser.add_argument('--test', action='store_true',
                       help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†5ä¸ªï¼‰')
    
    args = parser.parse_args()
    
    if args.test:
        args.target = 5
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†5ä¸ªLSOA")
    
    # è¿è¡Œæµå¼å¤„ç†
    pipeline = StreamPipeline()
    results = pipeline.run(target_count=args.target)
    
    print(f"\nâœ… å®Œæˆï¼å¤„ç†äº† {len(results)} ä¸ªLSOA")
    print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {Config.RESULTS_DIR}")
    print(f"ğŸ“Š å­˜å‚¨èŠ‚çœ: ~{args.target * 0.1:.1f}GBï¼ˆç›¸æ¯”ä¿å­˜æ‰€æœ‰å›¾ç‰‡ï¼‰")

if __name__ == "__main__":
    main()
