#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
City Look Dissertation v2 - å®Œæ•´ç‰ˆ
full_london_gvi_complete.py - æ•´åˆç°æœ‰æ•°æ®å¹¶ç»§ç»­æ”¶é›†

åŠŸèƒ½ï¼š
1. æ•´åˆå·²æœ‰çš„171ä¸ªLSOAç»“æœ
2. ç»§ç»­æ”¶é›†å‰©ä½™LSOAæ•°æ®
3. å¯¹nâ‰¥30çš„è®¡ç®—GVIï¼Œn<30çš„åªè®°å½•
4. æ‰€æœ‰ç»“æœä¿å­˜åˆ°ä¸€ä¸ªCSVæ–‡ä»¶
"""

import os
import sys
import json
import time
import math
import logging
import warnings
from io import BytesIO
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import requests
from PIL import Image, ImageFile
from tqdm import tqdm

warnings.filterwarnings("ignore")
ImageFile.LOAD_TRUNCATED_IMAGES = True

try:
    import geopandas as gpd
except Exception:
    gpd = None

try:
    import torch
except Exception:
    torch = None

# ============================================
# é…ç½®
# ============================================

class Config:
    """é¡¹ç›®é…ç½®"""
    BASE_DIR = Path(r"C:\Users\z1782\OneDrive - University College London\Attachments\004\methodology\Dissertation_v2")
    
    # æ•°æ®æ–‡ä»¶
    IMD_FILE = BASE_DIR / "data" / "raw" / "IMD 2019" / "IMD2019_London.xlsx"
    LSOA_BOUNDARIES = BASE_DIR / "data" / "raw" / "statistical-gis-boundaries-london" / "ESRI" / "LSOA_2011_London_gen_MHW.shp"
    
    # å·²æœ‰ç»“æœæ–‡ä»¶
    EXISTING_RESULTS = BASE_DIR / "output" / "full_london_gvi" / "inner_london_gvi_results_all_rows.csv"
    
    # è¾“å‡ºè·¯å¾„
    OUTPUT_DIR = BASE_DIR / "output" / "full_london_gvi"
    MAIN_OUTPUT_FILE = OUTPUT_DIR / "inner_london_all_lsoas_status.csv"  # ä¸»è¾“å‡ºæ–‡ä»¶
    CHECKPOINT_FILE = OUTPUT_DIR / "processing_checkpoint.json"
    DETAIL_DIR = OUTPUT_DIR / "image_details"
    
    # Mapillary
    MAPILLARY_TOKEN = os.getenv("MAPILLARY_TOKEN", "MLY|9922859457805691|cef02444f32c339cf09761b104ca4bb5")
    MAPILLARY_API = "https://graph.mapillary.com"
    
    # Inner London Boroughs
    INNER_LONDON_BOROUGHS = [
        "Camden", "Greenwich", "Hackney", "Hammersmith and Fulham",
        "Islington", "Kensington and Chelsea", "Lambeth", "Lewisham",
        "Southwark", "Tower Hamlets", "Wandsworth", "Westminster"
    ]
    
    # å¤„ç†å‚æ•°
    MIN_IMAGES_FOR_GVI = 30  # æœ€å°‘30å¼ æ‰è®¡ç®—GVI
    TARGET_IMAGES = 80        # ç›®æ ‡å›¾ç‰‡æ•°
    MAX_IMAGES = 100          # æœ€å¤šå¤„ç†å›¾ç‰‡æ•°
    
    # æ‰¹å¤„ç†
    BATCH_SIZE = 50           # æ¯æ‰¹å¤„ç†50ä¸ªLSOA
    MAX_WORKERS = 4           # å¹¶è¡Œçº¿ç¨‹æ•°
    
    # æ¨¡å‹å‚æ•°
    MODEL_SIZE = os.getenv("SEGFORMER_SIZE", "b1" if (torch and torch.cuda.is_available()) else "b0")
    
    # æœç´¢ç­–ç•¥
    SEARCH_RADIUS_LEVELS = [0.0, 0.1, 0.2, 0.3, 0.5]  # è¾¹ç•Œæ‰©å±•æ¯”ä¾‹
    TIME_RANGES = [
        ("2022-01-01", "2025-12-31"),
        ("2020-01-01", "2025-12-31"),
        ("2018-01-01", "2025-12-31"),
        ("2015-01-01", "2025-12-31"),
        (None, None)
    ]
    
    # åˆ—åå€™é€‰ï¼ˆç”¨äºè¯»å–IMDæ•°æ®ï¼‰
    IMD_LSOA_CODE_CANDS = ["LSOA code (2011)", "LSOA11CD", "lsoa11cd", "LSOA code", "LSOA_CODE"]
    IMD_LSOA_NAME_CANDS = ["LSOA name (2011)", "LSOA11NM", "lsoa11nm", "LSOA name"]
    IMD_SCORE_CANDS = ["Index of Multiple Deprivation (IMD) Score", "IMD Score", "imd_score"]
    IMD_BOROUGH_CANDS = ["Borough", "Local Authority District name (2019)", "Local Authority Name", "borough"]

# ============================================
# æ—¥å¿—è®¾ç½®
# ============================================

def setup_logging():
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    Config.DETAIL_DIR.mkdir(parents=True, exist_ok=True)
    
    log_file = Config.OUTPUT_DIR / f"processing_{datetime.now():%Y%m%d_%H%M}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

# ============================================
# æ•°æ®ç®¡ç†å™¨
# ============================================

class DataManager:
    def __init__(self, logger):
        self.logger = logger
        self.all_lsoas = None
        self.results_df = None
        self.processed_codes = set()
        
    def load_existing_results(self):
        """åŠ è½½å·²æœ‰çš„LSOAç»“æœ"""
        # ä¼˜å…ˆåŠ è½½ä¸»è¾“å‡ºæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰å·²å¤„ç†çš„ï¼‰
        if Config.MAIN_OUTPUT_FILE.exists():
            try:
                self.results_df = pd.read_csv(Config.MAIN_OUTPUT_FILE)
                self.processed_codes = set(self.results_df['lsoa_code'].values)
                self.logger.info(f"âœ… åŠ è½½å·²å¤„ç†ç»“æœï¼š{len(self.results_df)} ä¸ªLSOA")
                return True
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ä¸»æ–‡ä»¶ï¼š{e}")
        
        # å¦‚æœä¸»æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½åŸå§‹171ä¸ª
        if Config.EXISTING_RESULTS.exists():
            try:
                self.results_df = pd.read_csv(Config.EXISTING_RESULTS)
                self.results_df['status'] = 'completed'  # æ ‡è®°ä¸ºå·²å®Œæˆ
                self.processed_codes = set(self.results_df['lsoa_code'].values)
                self.logger.info(f"âœ… åŠ è½½åŸå§‹ç»“æœï¼š{len(self.results_df)} ä¸ªLSOA")
                
                # ä¿å­˜åˆ°ä¸»è¾“å‡ºæ–‡ä»¶
                self.results_df.to_csv(Config.MAIN_OUTPUT_FILE, index=False)
                return True
            except Exception as e:
                self.logger.error(f"åŠ è½½å·²æœ‰ç»“æœå¤±è´¥ï¼š{e}")
        
        self.logger.info("æœªæ‰¾åˆ°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œä»å¤´å¼€å§‹")
        self.results_df = pd.DataFrame()
        return False
    
    @staticmethod
    def _pick_col(df, cands):
        """ä»å€™é€‰åˆ—åä¸­é€‰æ‹©å­˜åœ¨çš„åˆ—"""
        for c in cands:
            if c in df.columns:
                return c
        low = [c.lower() for c in df.columns]
        for c in cands:
            for i, name in enumerate(low):
                if c.lower() == name:
                    return df.columns[i]
        return None
    
    def load_all_lsoas(self):
        """åŠ è½½æ‰€æœ‰Inner London LSOAs"""
        self.logger.info("åŠ è½½æ‰€æœ‰Inner London LSOAs...")
        
        if not Config.IMD_FILE.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°IMDæ–‡ä»¶ï¼š{Config.IMD_FILE}")
        
        imd = pd.read_excel(Config.IMD_FILE)
        
        # æ‰¾åˆ°æ­£ç¡®çš„åˆ—å
        code_col = self._pick_col(imd, Config.IMD_LSOA_CODE_CANDS)
        name_col = self._pick_col(imd, Config.IMD_LSOA_NAME_CANDS)
        score_col = self._pick_col(imd, Config.IMD_SCORE_CANDS)
        borough_col = self._pick_col(imd, Config.IMD_BOROUGH_CANDS)
        
        if not all([code_col, name_col, score_col, borough_col]):
            raise KeyError("IMDè¡¨ç¼ºå°‘å¿…è¦åˆ—")
        
        # ç­›é€‰Inner London
        inner = imd[imd[borough_col].isin(Config.INNER_LONDON_BOROUGHS)].copy()
        inner = inner.rename(columns={
            code_col: "lsoa_code",
            name_col: "lsoa_name",
            score_col: "imd_score",
            borough_col: "borough"
        })
        
        # è®¡ç®—IMDäº”åˆ†ä½
        inner["imd_quintile"] = pd.qcut(
            inner["imd_score"], q=5, 
            labels=["Q1_Least", "Q2", "Q3", "Q4", "Q5_Most"]
        )
        
        self.all_lsoas = inner[["lsoa_code", "lsoa_name", "borough", "imd_score", "imd_quintile"]]
        self.logger.info(f"æ‰¾åˆ° {len(self.all_lsoas)} ä¸ªInner London LSOAs")
        
        # è®¡ç®—å¾…å¤„ç†æ•°é‡
        remaining = len(self.all_lsoas) - len(self.processed_codes)
        self.logger.info(f"å·²å¤„ç†ï¼š{len(self.processed_codes)}ï¼Œå¾…å¤„ç†ï¼š{remaining}")
        
        return self.all_lsoas
    
    def get_unprocessed_lsoas(self):
        """è·å–æœªå¤„ç†çš„LSOAs"""
        unprocessed = self.all_lsoas[~self.all_lsoas['lsoa_code'].isin(self.processed_codes)]
        return unprocessed
    
    def save_result(self, result):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        # æ·»åŠ åˆ°DataFrame
        new_row = pd.DataFrame([result])
        
        if self.results_df is None or self.results_df.empty:
            self.results_df = new_row
        else:
            self.results_df = pd.concat([self.results_df, new_row], ignore_index=True)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        self.results_df.to_csv(Config.MAIN_OUTPUT_FILE, index=False)
        self.processed_codes.add(result['lsoa_code'])
        
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'processed': list(self.processed_codes),
            'timestamp': datetime.now().isoformat(),
            'total_processed': len(self.processed_codes)
        }
        with open(Config.CHECKPOINT_FILE, 'w') as f:
            json.dump(checkpoint, f, indent=2)

# ============================================
# Mapillary API
# ============================================

class MapillaryAPI:
    def __init__(self, token, logger):
        self.token = token
        self.logger = logger
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"OAuth {token}"})
        
    def search_images(self, bbox, min_date=None, max_date=None, limit=500):
        """æœç´¢æŒ‡å®šåŒºåŸŸçš„å›¾ç‰‡"""
        url = f"{Config.MAPILLARY_API}/images"
        params = {
            "bbox": f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}",
            "fields": "id,captured_at,computed_geometry,thumb_2048_url,thumb_1024_url,sequence",
            "limit": limit,
            "is_pano": "false"
        }
        
        if min_date:
            params["min_captured_at"] = f"{min_date}T00:00:00Z"
        if max_date:
            params["max_captured_at"] = f"{max_date}T23:59:59Z"
        
        try:
            resp = self.session.get(url, params=params, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                return data.get('data', [])
        except Exception as e:
            self.logger.error(f"Mapillary APIé”™è¯¯ï¼š{e}")
        
        return []
    
    def search_with_strategy(self, bbox, target=80):
        """ä½¿ç”¨å¤šç§ç­–ç•¥æœç´¢å›¾ç‰‡"""
        all_images = []
        seen_ids = set()
        
        # å°è¯•ä¸åŒçš„æ—¶é—´èŒƒå›´å’Œç©ºé—´æ‰©å±•
        for time_range in Config.TIME_RANGES:
            for expansion in Config.SEARCH_RADIUS_LEVELS:
                # æ‰©å±•è¾¹ç•Œ
                expanded_bbox = self._expand_bbox(bbox, expansion)
                
                # æœç´¢
                images = self.search_images(
                    expanded_bbox,
                    min_date=time_range[0] if time_range else None,
                    max_date=time_range[1] if time_range else None
                )
                
                # å»é‡æ·»åŠ 
                for img in images:
                    if img.get('id') not in seen_ids:
                        all_images.append(img)
                        seen_ids.add(img.get('id'))
                
                # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå›¾ç‰‡ï¼Œè¿”å›
                if len(all_images) >= target:
                    return all_images[:target]
        
        return all_images
    
    def _expand_bbox(self, bbox, factor):
        """æ‰©å±•è¾¹ç•Œæ¡†"""
        if factor == 0:
            return bbox
        
        center_lon = (bbox[0] + bbox[2]) / 2
        center_lat = (bbox[1] + bbox[3]) / 2
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        
        new_width = width * (1 + factor)
        new_height = height * (1 + factor)
        
        return [
            center_lon - new_width / 2,
            center_lat - new_height / 2,
            center_lon + new_width / 2,
            center_lat + new_height / 2
        ]
    
    def download_image(self, image_meta):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        url = image_meta.get('thumb_2048_url') or image_meta.get('thumb_1024_url')
        if not url:
            return None
        
        try:
            resp = self.session.get(url, timeout=30)
            if resp.status_code == 200:
                return Image.open(BytesIO(resp.content)).convert('RGB')
        except Exception:
            pass
        
        return None

# ============================================
# GVIè®¡ç®—å™¨
# ============================================

class GVICalculator:
    def __init__(self, model_size="b0", device=None):
        if torch is None:
            raise ImportError("éœ€è¦å®‰è£…PyTorch")
        
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.device = device
        
        # åŠ è½½æ¨¡å‹
        model_name = f"nvidia/segformer-{model_size}-finetuned-ade-512-512"
        
        try:
            from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
            self.processor = AutoImageProcessor.from_pretrained(model_name)
            self.model = AutoModelForSemanticSegmentation.from_pretrained(model_name)
        except Exception as e:
            raise ImportError(f"æ— æ³•åŠ è½½æ¨¡å‹ï¼š{e}")
        
        self.model.to(self.device)
        self.model.eval()
        
        # æ¤è¢«ç±»åˆ«ID
        self.vegetation_ids = {4, 9, 17, 29, 46, 66, 87, 90}
        print(f"ä½¿ç”¨è®¾å¤‡ï¼š{self.device}")
        print(f"æ¤è¢«ç±»åˆ«IDsï¼š{sorted(self.vegetation_ids)}")
    
    @torch.no_grad()
    def calculate_gvi(self, pil_image):
        """è®¡ç®—å•å¼ å›¾ç‰‡çš„GVI"""
        try:
            # è°ƒæ•´å¤§å°
            if max(pil_image.size) > 512:
                pil_image.thumbnail((512, 512), Image.Resampling.LANCZOS)
            
            # å¤„ç†å›¾ç‰‡
            inputs = self.processor(images=pil_image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # é¢„æµ‹
            outputs = self.model(**inputs)
            pred = outputs.logits.argmax(dim=1).cpu().numpy()[0]
            
            # è®¡ç®—GVI
            veg_mask = np.isin(pred, list(self.vegetation_ids))
            total_pixels = pred.size
            veg_pixels = veg_mask.sum()
            
            gvi = (veg_pixels / total_pixels) * 100.0
            
            return gvi
            
        except Exception:
            return None
        finally:
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

# ============================================
# LSOAå¤„ç†å™¨
# ============================================

class LSOAProcessor:
    def __init__(self, mapillary_api, gvi_calculator, logger):
        self.mapillary = mapillary_api
        self.calculator = gvi_calculator
        self.logger = logger
    
    def process_lsoa(self, lsoa_info):
        """å¤„ç†å•ä¸ªLSOA"""
        lsoa_code = lsoa_info['lsoa_code']
        t0 = time.time()
        
        self.logger.info(f"å¤„ç† {lsoa_code} - {lsoa_info['borough']}")
        
        # è·å–è¾¹ç•Œ
        bbox = self._get_bbox(lsoa_code)
        
        # æœç´¢å›¾ç‰‡
        images = self.mapillary.search_with_strategy(bbox, target=Config.TARGET_IMAGES)
        n_images = len(images)
        
        self.logger.info(f"  æ‰¾åˆ° {n_images} å¼ å›¾ç‰‡")
        
        # æ ¹æ®å›¾ç‰‡æ•°é‡å†³å®šå¤„ç†æ–¹å¼
        if n_images == 0:
            # æ— æ•°æ®
            result = {
                'lsoa_code': lsoa_code,
                'lsoa_name': lsoa_info.get('lsoa_name', ''),
                'borough': lsoa_info['borough'],
                'imd_score': lsoa_info['imd_score'],
                'imd_quintile': str(lsoa_info['imd_quintile']),
                'status': 'no_data',
                'n_images': 0,
                'mean_gvi': None,
                'median_gvi': None,
                'std_gvi': None,
                'min_gvi': None,
                'max_gvi': None,
                'q25_gvi': None,
                'q75_gvi': None,
                'processing_time': time.time() - t0,
                'timestamp': datetime.now().isoformat()
            }
            
        elif n_images < Config.MIN_IMAGES_FOR_GVI:
            # å›¾ç‰‡ä¸è¶³ï¼Œä¸è®¡ç®—GVI
            result = {
                'lsoa_code': lsoa_code,
                'lsoa_name': lsoa_info.get('lsoa_name', ''),
                'borough': lsoa_info['borough'],
                'imd_score': lsoa_info['imd_score'],
                'imd_quintile': str(lsoa_info['imd_quintile']),
                'status': 'insufficient',
                'n_images': n_images,
                'mean_gvi': None,
                'median_gvi': None,
                'std_gvi': None,
                'min_gvi': None,
                'max_gvi': None,
                'q25_gvi': None,
                'q75_gvi': None,
                'processing_time': time.time() - t0,
                'timestamp': datetime.now().isoformat()
            }
            
        else:
            # è®¡ç®—GVI
            gvi_values = []
            processed = 0
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = []
                for img_meta in images[:Config.MAX_IMAGES]:
                    future = executor.submit(self._process_single_image, img_meta)
                    futures.append(future)
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    gvi = future.result()
                    if gvi is not None:
                        gvi_values.append(gvi)
                        processed += 1
                    
                    # è¾¾åˆ°ç›®æ ‡æ•°é‡å³å¯
                    if processed >= Config.TARGET_IMAGES:
                        break
            
            # è®¡ç®—ç»Ÿè®¡å€¼
            if len(gvi_values) >= Config.MIN_IMAGES_FOR_GVI:
                gvi_array = np.array(gvi_values)
                result = {
                    'lsoa_code': lsoa_code,
                    'lsoa_name': lsoa_info.get('lsoa_name', ''),
                    'borough': lsoa_info['borough'],
                    'imd_score': lsoa_info['imd_score'],
                    'imd_quintile': str(lsoa_info['imd_quintile']),
                    'status': 'completed',
                    'n_images': len(gvi_values),
                    'mean_gvi': float(np.mean(gvi_array)),
                    'median_gvi': float(np.median(gvi_array)),
                    'std_gvi': float(np.std(gvi_array)),
                    'min_gvi': float(np.min(gvi_array)),
                    'max_gvi': float(np.max(gvi_array)),
                    'q25_gvi': float(np.percentile(gvi_array, 25)),
                    'q75_gvi': float(np.percentile(gvi_array, 75)),
                    'processing_time': time.time() - t0,
                    'timestamp': datetime.now().isoformat()
                }
                self.logger.info(f"  âœ… GVI = {result['mean_gvi']:.2f}%")
            else:
                # å¤„ç†å¤±è´¥
                result = {
                    'lsoa_code': lsoa_code,
                    'lsoa_name': lsoa_info.get('lsoa_name', ''),
                    'borough': lsoa_info['borough'],
                    'imd_score': lsoa_info['imd_score'],
                    'imd_quintile': str(lsoa_info['imd_quintile']),
                    'status': 'insufficient',
                    'n_images': n_images,
                    'mean_gvi': None,
                    'median_gvi': None,
                    'std_gvi': None,
                    'min_gvi': None,
                    'max_gvi': None,
                    'q25_gvi': None,
                    'q75_gvi': None,
                    'processing_time': time.time() - t0,
                    'timestamp': datetime.now().isoformat()
                }
        
        return result
    
    def _process_single_image(self, img_meta):
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        try:
            # ä¸‹è½½å›¾ç‰‡
            img = self.mapillary.download_image(img_meta)
            if img is None:
                return None
            
            # è®¡ç®—GVI
            gvi = self.calculator.calculate_gvi(img)
            
            # æ¸…ç†
            img.close()
            
            return gvi
            
        except Exception:
            return None
    
    def _get_bbox(self, lsoa_code):
        """è·å–LSOAè¾¹ç•Œæ¡†"""
        try:
            if gpd and Config.LSOA_BOUNDARIES.exists():
                gdf = gpd.read_file(Config.LSOA_BOUNDARIES)
                gdf = gdf.to_crs(epsg=4326)
                
                lsoa_row = gdf[gdf['LSOA11CD'] == lsoa_code]
                if not lsoa_row.empty:
                    bounds = lsoa_row.total_bounds
                    return [bounds[0], bounds[1], bounds[2], bounds[3]]
        except Exception:
            pass
        
        # é»˜è®¤è¾¹ç•Œï¼ˆä¼¦æ•¦ä¸­å¿ƒï¼‰
        return [-0.15, 51.48, -0.05, 51.54]

# ============================================
# ä¸»å¤„ç†ç®¡é“
# ============================================

class MainPipeline:
    def __init__(self):
        self.logger = setup_logging()
        self.data_manager = DataManager(self.logger)
        self.mapillary = MapillaryAPI(Config.MAPILLARY_TOKEN, self.logger)
        
        # åˆå§‹åŒ–GVIè®¡ç®—å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            self.calculator = GVICalculator(model_size=Config.MODEL_SIZE)
            self.gvi_enabled = True
        except Exception as e:
            self.logger.warning(f"GVIè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            self.logger.warning("å°†åªè®°å½•å›¾ç‰‡æ•°é‡ï¼Œä¸è®¡ç®—GVI")
            self.calculator = None
            self.gvi_enabled = False
        
        self.processor = LSOAProcessor(self.mapillary, self.calculator, self.logger)
    
    def run(self, batch_size=50, max_lsoas=None):
        """è¿è¡Œå¤„ç†æµç¨‹"""
        self.logger.info("=" * 70)
        self.logger.info("Inner London LSOA æ•°æ®æ”¶é›†")
        self.logger.info("=" * 70)
        
        # åŠ è½½å·²æœ‰ç»“æœ
        self.data_manager.load_existing_results()
        
        # åŠ è½½æ‰€æœ‰LSOA
        self.data_manager.load_all_lsoas()
        
        # è·å–æœªå¤„ç†çš„LSOA
        unprocessed = self.data_manager.get_unprocessed_lsoas()
        
        if max_lsoas:
            unprocessed = unprocessed.head(max_lsoas)
            self.logger.info(f"é™åˆ¶å¤„ç†æ•°é‡ï¼š{max_lsoas}")
        
        total_to_process = len(unprocessed)
        
        if total_to_process == 0:
            self.logger.info("æ‰€æœ‰LSOAå·²å¤„ç†å®Œæˆï¼")
            return
        
        self.logger.info(f"å¾…å¤„ç†ï¼š{total_to_process} ä¸ªLSOA")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°ï¼š{batch_size}")
        
        # åˆ†æ‰¹å¤„ç†
        processed_count = 0
        
        for batch_start in range(0, total_to_process, batch_size):
            batch_end = min(batch_start + batch_size, total_to_process)
            batch = unprocessed.iloc[batch_start:batch_end]
            
            self.logger.info(f"\næ‰¹æ¬¡ {batch_start//batch_size + 1}ï¼šå¤„ç† {batch_start+1}-{batch_end}/{total_to_process}")
            
            for _, lsoa_info in batch.iterrows():
                processed_count += 1
                
                self.logger.info(f"\n[{processed_count}/{total_to_process}] {lsoa_info['lsoa_code']}")
                
                try:
                    # å¤„ç†LSOA
                    result = self.processor.process_lsoa(lsoa_info)
                    
                    # ä¿å­˜ç»“æœ
                    self.data_manager.save_result(result)
                    
                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if processed_count % 10 == 0:
                        self.data_manager.save_checkpoint()
                        self._report_progress()
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†å¤±è´¥ {lsoa_info['lsoa_code']}: {e}")
                    # è®°å½•å¤±è´¥
                    failed_result = {
                        'lsoa_code': lsoa_info['lsoa_code'],
                        'lsoa_name': lsoa_info.get('lsoa_name', ''),
                        'borough': lsoa_info['borough'],
                        'imd_score': lsoa_info['imd_score'],
                        'imd_quintile': str(lsoa_info['imd_quintile']),
                        'status': 'error',
                        'n_images': 0,
                        'mean_gvi': None,
                        'median_gvi': None,
                        'std_gvi': None,
                        'min_gvi': None,
                        'max_gvi': None,
                        'q25_gvi': None,
                        'q75_gvi': None,
                        'processing_time': 0,
                        'timestamp': datetime.now().isoformat()
                    }
                    self.data_manager.save_result(failed_result)
        
        # æœ€ç»ˆæŠ¥å‘Š
        self._final_report()
    
    def _report_progress(self):
        """æŠ¥å‘Šè¿›åº¦"""
        df = self.data_manager.results_df
        if df is not None and len(df) > 0:
            completed = len(df[df['status'] == 'completed'])
            insufficient = len(df[df['status'] == 'insufficient'])
            no_data = len(df[df['status'] == 'no_data'])
            
            self.logger.info("\n--- è¿›åº¦æŠ¥å‘Š ---")
            self.logger.info(f"å·²å®Œæˆ(GVIè®¡ç®—): {completed}")
            self.logger.info(f"æ•°æ®ä¸è¶³(<30): {insufficient}")
            self.logger.info(f"æ— æ•°æ®: {no_data}")
            self.logger.info(f"æ€»è®¡: {len(df)}")
    
    def _final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("å¤„ç†å®Œæˆï¼")
        
        df = self.data_manager.results_df
        if df is not None and len(df) > 0:
            # ç»Ÿè®¡
            stats = {
                'total': len(df),
                'completed': len(df[df['status'] == 'completed']),
                'insufficient': len(df[df['status'] == 'insufficient']),
                'no_data': len(df[df['status'] == 'no_data']),
                'error': len(df[df['status'] == 'error']) if 'error' in df['status'].values else 0
            }
            
            self.logger.info(f"\næœ€ç»ˆç»Ÿè®¡ï¼š")
            self.logger.info(f"  æ€»LSOAæ•°: {stats['total']}")
            self.logger.info(f"  å·²è®¡ç®—GVI: {stats['completed']} ({stats['completed']/stats['total']*100:.1f}%)")
            self.logger.info(f"  æ•°æ®ä¸è¶³: {stats['insufficient']} ({stats['insufficient']/stats['total']*100:.1f}%)")
            self.logger.info(f"  æ— æ•°æ®: {stats['no_data']} ({stats['no_data']/stats['total']*100:.1f}%)")
            
            # å¦‚æœæœ‰GVIæ•°æ®ï¼Œè®¡ç®—å¹³å‡å€¼
            completed_df = df[df['status'] == 'completed']
            if len(completed_df) > 0:
                mean_gvi = completed_df['mean_gvi'].mean()
                self.logger.info(f"\nå¹³å‡GVI: {mean_gvi:.2f}%")
            
            # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
            report_file = Config.OUTPUT_DIR / f"final_report_{datetime.now():%Y%m%d_%H%M}.json"
            with open(report_file, 'w') as f:
                json.dump(stats, f, indent=2)
            
            self.logger.info(f"\nè¾“å‡ºæ–‡ä»¶ï¼š")
            self.logger.info(f"  ä¸»æ•°æ®æ–‡ä»¶: {Config.MAIN_OUTPUT_FILE}")
            self.logger.info(f"  ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
        
        self.logger.info("=" * 70)

# ============================================
# ä¸»ç¨‹åº
# ============================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Inner London LSOA GVIæ•°æ®æ”¶é›†")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆ5ä¸ªLSOAï¼‰")
    parser.add_argument("--batch-size", type=int, default=50, help="æ¯æ‰¹å¤„ç†çš„LSOAæ•°é‡")
    parser.add_argument("--max-lsoas", type=int, default=None, help="æœ€å¤§å¤„ç†LSOAæ•°")
    
    args = parser.parse_args()
    
    if args.test:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†5ä¸ªLSOA")
        args.max_lsoas = 5
        args.batch_size = 5
    
    print(f"é…ç½®ï¼š")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - æœ€å¤§å¤„ç†æ•°: {args.max_lsoas if args.max_lsoas else 'å…¨éƒ¨'}")
    print(f"  - GVIè®¡ç®—é˜ˆå€¼: â‰¥{Config.MIN_IMAGES_FOR_GVI}å¼ å›¾ç‰‡")
    print()
    
    # è¿è¡Œ
    pipeline = MainPipeline()
    pipeline.run(batch_size=args.batch_size, max_lsoas=args.max_lsoas)

if __name__ == "__main__":
    main()
